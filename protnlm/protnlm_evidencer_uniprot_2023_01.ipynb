{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Copyright 2023 Google Inc.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "```"
      ],
      "metadata": {
        "id": "Nc5z20tucbYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab supports UniProt 2023_01, where Google predicted protein names for 10s of millions of proteins previously named \"Uncharacterized protein\".\n",
        "\n",
        "You can run this file to check whether any prediction (especially for previously \"Uncharacterized\" proteins) produced by Google's systems is supported by other sources.\n",
        "\n",
        "\n",
        "**Paste in the UniProt accession of the protein below!**"
      ],
      "metadata": {
        "id": "NrIKMuHM_pS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and dependencies"
      ],
      "metadata": {
        "id": "yvGAanHAsf04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install pigz"
      ],
      "metadata": {
        "id": "o43XlscFsCIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aadac880-00cd-4bdd-bca7-239c0ff3dbf4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  pigz\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 57.4 kB of archives.\n",
            "After this operation, 259 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 pigz amd64 2.4-1 [57.4 kB]\n",
            "Fetched 57.4 kB in 0s (136 kB/s)\n",
            "Selecting previously unselected package pigz.\n",
            "(Reading database ... 128221 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/pigz_2.4-1_amd64.deb ...\n",
            "Unpacking pigz (2.4-1) ...\n",
            "Setting up pigz (2.4-1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install binary_file_search"
      ],
      "metadata": {
        "id": "YhaVGrgU98Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d24593f-0ef8-4433-9e8e-82f4082de04e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting binary_file_search\n",
            "  Downloading binary_file_search-0.7.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: binary_file_search\n",
            "  Building wheel for binary_file_search (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for binary_file_search: filename=binary_file_search-0.7-py3-none-any.whl size=3968 sha256=b141c26e8f9b959d7b36c1dbc2dca0db139009d8f159469904c667a92bd7a28e\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/6d/75/59fc28bd86128edfe2b5a03ebfb32dcd0016e6e10ba8d7fc04\n",
            "Successfully built binary_file_search\n",
            "Installing collected packages: binary_file_search\n",
            "Successfully installed binary_file_search-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ymlaxuFFWFlE"
      },
      "outputs": [],
      "source": [
        "from binary_file_search.BinaryFileSearch import BinaryFileSearch\n",
        "import IPython.display\n",
        "def print_markdown(string):\n",
        "    IPython.display.display(IPython.display.Markdown(string))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download evidence file and unzip\n",
        "(takes a few minutes)"
      ],
      "metadata": {
        "id": "2NbvPJy2sjFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c -O sorted_evidencer.csv.gz https://storage.googleapis.com/brain-genomics-public/research/proteins/protnlm/uniprot_2023_01_evidencer_sorted.csv.gz"
      ],
      "metadata": {
        "id": "DC0CFu7m9tDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99373fd0-d340-40bd-a14a-327affdb89ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-28 20:01:38--  https://storage.googleapis.com/brain-genomics-public/research/proteins/protnlm/uniprot_2023_01_evidencer_sorted.csv.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.98.128, 74.125.197.128, 74.125.135.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.98.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 317356364 (303M) [application/octet-stream]\n",
            "Saving to: ‘sorted_evidencer.csv.gz’\n",
            "\n",
            "sorted_evidencer.cs 100%[===================>] 302.65M  52.3MB/s    in 5.7s    \n",
            "\n",
            "2023-02-28 20:01:44 (53.3 MB/s) - ‘sorted_evidencer.csv.gz’ saved [317356364/317356364]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unpigz -fk sorted_evidencer.csv.gz"
      ],
      "metadata": {
        "id": "g7crO7tz919X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search for your evidence"
      ],
      "metadata": {
        "id": "cU4GrPdE_SfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accession = \"A0A3M3H8U9\" #@param {type:\"string\"}\n",
        "with BinaryFileSearch('sorted_evidencer.csv', sep=\",\", string_mode=True) as bfs:\n",
        "  try:\n",
        "    lines = bfs.search(accession)\n",
        "  except KeyError:\n",
        "    raise ValueError('Sorry, this protein\\'s accession wasn\\'t found in our database. Maybe check your spelling, or maybe this prediction wasn\\'t provided by Google?')\n",
        "\n",
        "if len(lines) > 1:\n",
        "  for l in lines:\n",
        "    print(l)\n",
        "  raise ValueError('There was some sort of error - we found multiple predictions for this protein!', lines)\n",
        "\n",
        "accession, prediction, _, alignment_support, structure_support = tuple(lines[0])\n",
        "\n",
        "to_print_prefix = f'The prediction **{prediction}** for **{accession}**: \\n'\n",
        "to_print = to_print_prefix\n",
        "if alignment_support:\n",
        "  to_add = f\"* has a strong phmmer alignment to **{alignment_support}** (bit score > 25).\\n\"\n",
        "  to_print += to_add\n",
        "if structure_support:\n",
        "  to_add = f\"* has a structural alignment to the high-confidence AlphaFold structure for **{alignment_support}** (tmalign score > .5).\\n\"\n",
        "  to_print += to_add\n",
        "\n",
        "if to_print == to_print_prefix:\n",
        "  to_print += f\"* **no support** found with these automated methods. Perhaps the protein is very new or the prediction is wrong?\"\n",
        "\n",
        "print_markdown(to_print)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "cellView": "form",
        "id": "N17c3oC39_0e",
        "outputId": "f412bab8-e1c9-4d83-d8dd-a07cea19f913"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The prediction **ATPase** for **A0A3M3H8U9**: \n* has a strong phmmer alignment to **A0A345UY82** (bit score > 25).\n* has a structural alignment to the high-confidence AlphaFold structure for **A0A345UY82** (tmalign score > .5).\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# License of downloaded data\n",
        "\n",
        "The evidencer file is licensed CC-BY 4.0 and is built based on UniProt.\n",
        "\n",
        "\"UniProt: the universal protein knowledgebase in 2021.\" Nucleic acids research 49, no. D1 (2021): D480-D489."
      ],
      "metadata": {
        "id": "jnCOhzbTJ1RM"
      }
    }
  ]
}